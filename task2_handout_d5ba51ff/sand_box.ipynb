{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playin around with the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "from util import ece, ParameterDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `EXTENDED_EVALUATION` to `True` in order to visualize your predictions.\n",
    "EXTENDED_EVALUATION = False\n",
    "\n",
    "\n",
    "def run_solution(dataset_train: torch.utils.data.Dataset, data_dir: str = os.curdir, output_dir: str = '/results/') -> 'Model':\n",
    "    \"\"\"\n",
    "    Run your task 2 solution.\n",
    "    This method should train your model, evaluate it, and return the trained model at the end.\n",
    "    Make sure to preserve the method signature and to return your trained model,\n",
    "    else the checker will fail!\n",
    "\n",
    "    :param dataset_train: Training dataset\n",
    "    :param data_dir: Directory containing the datasets\n",
    "    :return: Your trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # Create model\n",
    "    model = Model()\n",
    "\n",
    "    # Train the model\n",
    "    print('Training model')\n",
    "    model.train(dataset_train)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    print('Evaluating model on training data')\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=64, shuffle=False, drop_last=False\n",
    "    )\n",
    "    evaluate(model, eval_loader, data_dir, output_dir)\n",
    "\n",
    "    # IMPORTANT: return your model here!\n",
    "    return model\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"\n",
    "    Task 2 model that can be used to train a BNN using Bayes by backprop and create predictions.\n",
    "    You need to implement all methods of this class without changing their signature,\n",
    "    else the checker will fail!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Hyperparameters and general parameters\n",
    "        # You might want to play around with those\n",
    "        self.num_epochs = 100  # number of training epochs\n",
    "        self.batch_size = 128  # training batch size\n",
    "        learning_rate = 1e-3  # training learning rates\n",
    "        hidden_layers = (100, 100)  # for each entry, creates a hidden layer with the corresponding number of units\n",
    "        use_densenet = False  # set this to True in order to run a DenseNet for comparison\n",
    "        self.print_interval = 100  # number of batches until updated metrics are displayed during training\n",
    "\n",
    "        # Determine network type\n",
    "        if use_densenet:\n",
    "            # DenseNet\n",
    "            print('Using a DenseNet model for comparison')\n",
    "            self.network = DenseNet(in_features=28 * 28, hidden_features=hidden_layers, out_features=10)\n",
    "        else:\n",
    "            # BayesNet\n",
    "            print('Using a BayesNet model')\n",
    "            self.network = BayesNet(in_features=28 * 28, hidden_features=hidden_layers, out_features=10)\n",
    "\n",
    "        # Optimizer for training\n",
    "        # Feel free to try out different optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(self, dataset: torch.utils.data.Dataset):\n",
    "        \"\"\"\n",
    "        Train your neural network.\n",
    "        If the network is a DenseNet, this performs normal stochastic gradient descent training.\n",
    "        If the network is a BayesNet, this should perform Bayes by backprop.\n",
    "\n",
    "        :param dataset: Dataset you should use for training\n",
    "        \"\"\"\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "\n",
    "        self.network.train()\n",
    "\n",
    "        progress_bar = trange(self.num_epochs)\n",
    "        for _ in progress_bar:\n",
    "            num_batches = len(train_loader)\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "                # batch_x are of shape (batch_size, 784), batch_y are of shape (batch_size,)\n",
    "\n",
    "                self.network.zero_grad()\n",
    "\n",
    "                if isinstance(self.network, DenseNet):\n",
    "                    # DenseNet training step\n",
    "\n",
    "                    # Perform forward pass\n",
    "                    current_logits = self.network(batch_x)\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    # We use the negative log likelihood as the loss\n",
    "                    # Combining nll_loss with a log_softmax is better for numeric stability\n",
    "                    loss = F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y, reduction='sum')\n",
    "\n",
    "                    # Backpropagate to get the gradients\n",
    "                    loss.backward()\n",
    "                else:\n",
    "                    # BayesNet training step via Bayes by backprop\n",
    "                    assert isinstance(self.network, BayesNet)\n",
    "\n",
    "                    # TODO: Implement Bayes by backprop training here\n",
    "                    # Perform forward pass\n",
    "                    current_logits, log_prior, log_variational_posterior  = self.network(batch_x)\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    # We use the negative log likelihood as the loss\n",
    "                    # Combining nll_loss with a log_softmax is better for numeric stability\n",
    "                    loss = log_variational_posterior - log_prior + F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y, reduction='sum')\n",
    "                    \n",
    "                    # Backpropagate to get the gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update progress bar with accuracy occasionally\n",
    "                if batch_idx % self.print_interval == 0:\n",
    "                    if isinstance(self.network, DenseNet):\n",
    "                        current_logits = self.network(batch_x)\n",
    "                    else:\n",
    "                        assert isinstance(self.network, BayesNet)\n",
    "                        current_logits, _, _ = self.network(batch_x)\n",
    "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
    "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item())\n",
    "\n",
    "    def predict(self, data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the class probabilities using your trained model.\n",
    "        This method should return an (num_samples, 10) NumPy float array\n",
    "        such that the second dimension sums up to 1 for each row.\n",
    "\n",
    "        :param data_loader: Data loader yielding the samples to predict on\n",
    "        :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.eval()\n",
    "\n",
    "        probability_batches = []\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            current_probabilities = self.network.predict_probabilities(batch_x).detach().numpy()\n",
    "            probability_batches.append(current_probabilities)\n",
    "\n",
    "        output = np.concatenate(probability_batches, axis=0)\n",
    "        assert isinstance(output, np.ndarray)\n",
    "        assert output.ndim == 2 and output.shape[1] == 10\n",
    "        assert np.allclose(np.sum(output, axis=1), 1.0)\n",
    "        return output\n",
    "\n",
    "\n",
    "class BayesianLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a single Bayesian feedforward layer.\n",
    "    It maintains a prior and variational posterior for the weights (and biases)\n",
    "    and uses sampling to approximate the gradients via Bayes by backprop.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        \"\"\"\n",
    "        Create a BayesianLayer.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param out_features: Number of output features\n",
    "        :param bias: If true, use a bias term (i.e., affine instead of linear transformation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # TODO: Create a suitable prior for weights and biases as an instance of ParameterDistribution.\n",
    "        #  You can use the same prior for both weights and biases, but are free to experiment with different priors.\n",
    "        #  You can create constants using torch.tensor(...).\n",
    "        #  Do NOT use torch.Parameter(...) here since the prior should not be optimized!\n",
    "        #  Example: self.prior = MyPrior(torch.tensor(0.0), torch.tensor(1.0))\n",
    "        self.prior = UnivariateGaussian(torch.tensor(0.0), torch.tensor(1.0))\n",
    "        assert isinstance(self.prior, ParameterDistribution)\n",
    "        assert not any(True for _ in self.prior.parameters()), 'Prior cannot have parameters'\n",
    "\n",
    "        # TODO: Create a suitable variational posterior for weights as an instance of ParameterDistribution.\n",
    "        #  You need to create separate ParameterDistribution instances for weights and biases,\n",
    "        #  but can use the same family of distributions if you want.\n",
    "        #  IMPORTANT: You need to create a nn.Parameter(...) for each parameter\n",
    "        #  and add those parameters as an attribute in the ParameterDistribution instances.\n",
    "        #  If you forget to do so, PyTorch will not be able to optimize your variational posterior.\n",
    "        #  Example: self.weights_var_posterior = MyPosterior(\n",
    "        #      torch.nn.Parameter(torch.zeros((out_features, in_features))),\n",
    "        #      torch.nn.Parameter(torch.ones((out_features, in_features)))\n",
    "        #  )\n",
    "        self.weights_var_posterior = MultivariateDiagonalGaussian(\n",
    "            torch.nn.Parameter(torch.zeros((self.out_features, self.in_features))),\n",
    "            torch.nn.Parameter(torch.ones((self.out_features, self.in_features)))\n",
    "        )\n",
    "\n",
    "        assert isinstance(self.weights_var_posterior, ParameterDistribution)\n",
    "        assert any(True for _ in self.weights_var_posterior.parameters()), 'Weight posterior must have parameters'\n",
    "\n",
    "        if self.use_bias:\n",
    "            # TODO: As for the weights, create the bias variational posterior instance here.\n",
    "            #  Make sure to follow the same rules as for the weight variational posterior.\n",
    "            self.bias_var_posterior = MultivariateDiagonalGaussian(\n",
    "                torch.nn.Parameter(torch.zeros((self.out_features))),\n",
    "                torch.nn.Parameter(torch.ones((self.out_features)))\n",
    "            )\n",
    "            assert isinstance(self.bias_var_posterior, ParameterDistribution)\n",
    "            assert any(True for _ in self.bias_var_posterior.parameters()), 'Bias posterior must have parameters'\n",
    "        else:\n",
    "            self.bias_var_posterior = None\n",
    "            \n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Perform one forward pass through this layer.\n",
    "        If you need to sample weights from the variational posterior, you can do it here during the forward pass.\n",
    "        Just make sure that you use the same weights to approximate all quantities\n",
    "        present in a single Bayes by backprop sampling step.\n",
    "\n",
    "        :param inputs: Flattened input images as a (batch_size, in_features) float tensor\n",
    "        :return: 3-tuple containing\n",
    "            i) transformed features using stochastic weights from the variational posterior,\n",
    "            ii) sample of the log-prior probability, and\n",
    "            iii) sample of the log-variational-posterior probability\n",
    "        \"\"\"\n",
    "        # TODO: Perform a forward pass as described in this method's docstring.\n",
    "        #  Make sure to check whether `self.use_bias` is True,\n",
    "        #  and if yes, include the bias as well.\n",
    "        weights = self.weights_var_posterior.sample()\n",
    "        \n",
    "        if self.use_bias:\n",
    "            bias = self.bias_var_posterior.sample()\n",
    "            log_variational_posterior = self.weights_var_posterior.log_likelihood(weights) + self.bias_var_posterior.log_likelihood(bias)\n",
    "            log_prior = self.prior.log_likelihood(weights) + self.prior.log_likelihood(bias)\n",
    "        \n",
    "        else:\n",
    "            bias = None\n",
    "            log_variational_posterior = self.weights_var_posterior.log_likelihood(weights)\n",
    "            log_prior = self.prior.log_likelihood(weights)\n",
    "        \n",
    "        return F.linear(inputs, weights, bias), log_prior, log_variational_posterior\n",
    "\n",
    "\n",
    "class BayesNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a Bayesian feedforward neural network using BayesianLayer objects.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_features: typing.Tuple[int, ...], out_features: int):\n",
    "        \"\"\"\n",
    "        Create a BNN.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param hidden_features: Tuple where each entry corresponds to a (Bayesian) hidden layer with\n",
    "            the corresponding number of features.\n",
    "        :param out_features: Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        feature_sizes = (in_features,) + hidden_features + (out_features,)\n",
    "        num_affine_maps = len(feature_sizes) - 1\n",
    "        self.layers = nn.ModuleList([\n",
    "            BayesianLayer(feature_sizes[idx], feature_sizes[idx + 1], bias=True)\n",
    "            for idx in range(num_affine_maps)\n",
    "        ])\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Perform one forward pass through the BNN using a single set of weights\n",
    "        sampled from the variational posterior.\n",
    "\n",
    "        :param x: Input features, float tensor of shape (batch_size, in_features)\n",
    "        :return: 3-tuple containing\n",
    "            i) output features using stochastic weights from the variational posterior,\n",
    "            ii) sample of the log-prior probability, and\n",
    "            iii) sample of the log-variational-posterior probability\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Perform a full pass through your BayesNet as described in this method's docstring.\n",
    "        #  You can look at DenseNet to get an idea how a forward pass might look like.\n",
    "        #  Don't forget to apply your activation function in between BayesianLayers!\n",
    "        current_features = x\n",
    "        log_prior = torch.tensor(0.0)\n",
    "        log_variational_posterior = torch.tensor(0.0)\n",
    "\n",
    "        for idx, current_layer in enumerate(self.layers):\n",
    "            new_features, new_log_prior, new_log_variational_posterior  = current_layer(current_features)\n",
    "            if idx < len(self.layers) - 1:\n",
    "                new_features = self.activation(new_features)\n",
    "            current_features = new_features\n",
    "            log_prior =+ new_log_prior\n",
    "            log_variational_posterior =+ new_log_variational_posterior\n",
    "            \n",
    "        output_features = new_features\n",
    "\n",
    "        return output_features, log_prior, log_variational_posterior\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor, num_mc_samples: int = 10) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for the given features by sampling from this BNN.\n",
    "\n",
    "        :param x: Features to predict on, float tensor of shape (batch_size, in_features)\n",
    "        :param num_mc_samples: Number of MC samples to take for prediction\n",
    "        :return: Predicted class probabilities, float tensor of shape (batch_size, 10)\n",
    "            such that the last dimension sums up to 1 for each row\n",
    "        \"\"\"\n",
    "        probability_samples = torch.stack([F.softmax(self.forward(x)[0], dim=1) for _ in range(num_mc_samples)], dim=0)\n",
    "        estimated_probability = torch.mean(probability_samples, dim=0)\n",
    "\n",
    "        assert estimated_probability.shape == (x.shape[0], 10)\n",
    "        assert torch.allclose(torch.sum(estimated_probability, dim=1), torch.tensor(1.0))\n",
    "        return estimated_probability\n",
    "\n",
    "\n",
    "class UnivariateGaussian(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Univariate Gaussian distribution.\n",
    "    For multivariate data, this assumes all elements to be i.i.d.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, sigma: torch.Tensor):\n",
    "        super(UnivariateGaussian, self).__init__()  # always make sure to include the super-class init call!\n",
    "        assert mu.size() == () and sigma.size() == ()\n",
    "        assert sigma > 0\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement this\n",
    "        return torch.distributions.Normal(self.mu, self.sigma).log_prob(values).sum()\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        # TODO: Implement this\n",
    "        return torch.normal(self.mu, self.sigma)\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "\n",
    "class MultivariateDiagonalGaussian(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Multivariate diagonal Gaussian distribution,\n",
    "    i.e., assumes all elements to be independent Gaussians\n",
    "    but with different means and standard deviations.\n",
    "    This parameterizes the standard deviation via a parameter rho as\n",
    "    sigma = softplus(rho).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor):\n",
    "        super(MultivariateDiagonalGaussian, self).__init__()  # always make sure to include the super-class init call!\n",
    "        assert mu.size() == rho.size()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement this\n",
    "        return torch.distributions.Normal(self.mu.data, torch.log(1 + torch.exp(self.rho))).log_prob(values).sum()\n",
    "\n",
    "    \n",
    "    def sample(self) -> torch.Tensor:\n",
    "        # TODO: Implement this\n",
    "        return torch.normal(self.mu.data, torch.log(1 + torch.exp(self.rho)))\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "\n",
    "def evaluate(model: Model, eval_loader: torch.utils.data.DataLoader, data_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Evaluate your model.\n",
    "    :param model: Trained model to evaluate\n",
    "    :param eval_loader: Data loader containing the training set for evaluation\n",
    "    :param data_dir: Data directory from which additional datasets are loaded\n",
    "    :param output_dir: Directory into which plots are saved\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict class probabilities on test data\n",
    "    predicted_probabilities = model.predict(eval_loader)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "    actual_classes = eval_loader.dataset.tensors[1].detach().numpy()\n",
    "    accuracy = np.mean((predicted_classes == actual_classes))\n",
    "    ece_score = ece(predicted_probabilities, actual_classes)\n",
    "    print(f'Accuracy: {accuracy.item():.3f}, ECE score: {ece_score:.3f}')\n",
    "\n",
    "    if EXTENDED_EVALUATION:\n",
    "        eval_samples = eval_loader.dataset.tensors[0].detach().numpy()\n",
    "\n",
    "        # Determine confidence per sample and sort accordingly\n",
    "        confidences = np.max(predicted_probabilities, axis=1)\n",
    "        sorted_confidence_indices = np.argsort(confidences)\n",
    "\n",
    "        # Plot samples your model is most confident about\n",
    "        print('Plotting most confident MNIST predictions')\n",
    "        most_confident_indices = sorted_confidence_indices[-10:]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = most_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(np.reshape(eval_samples[sample_idx], (28, 28)), cmap='gray')\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f'predicted {predicted_classes[sample_idx]}, actual {actual_classes[sample_idx]}')\n",
    "                bar_colors = ['C0'] * 10\n",
    "                bar_colors[actual_classes[sample_idx]] = 'C1'\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(10), predicted_probabilities[sample_idx], tick_label=np.arange(10), color=bar_colors\n",
    "                )\n",
    "        fig.suptitle('Most confident predictions', size=20)\n",
    "        fig.savefig(os.path.join(output_dir, 'mnist_most_confident.pdf'))\n",
    "\n",
    "        # Plot samples your model is least confident about\n",
    "        print('Plotting least confident MNIST predictions')\n",
    "        least_confident_indices = sorted_confidence_indices[:10]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = least_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(np.reshape(eval_samples[sample_idx], (28, 28)), cmap='gray')\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f'predicted {predicted_classes[sample_idx]}, actual {actual_classes[sample_idx]}')\n",
    "                bar_colors = ['C0'] * 10\n",
    "                bar_colors[actual_classes[sample_idx]] = 'C1'\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(10), predicted_probabilities[sample_idx], tick_label=np.arange(10), color=bar_colors\n",
    "                )\n",
    "        fig.suptitle('Least confident predictions', size=20)\n",
    "        fig.savefig(os.path.join(output_dir, 'mnist_least_confident.pdf'))\n",
    "\n",
    "        print('Plotting ambiguous and rotated MNIST confidences')\n",
    "        ambiguous_samples = torch.from_numpy(np.load(os.path.join(data_dir, 'test_x.npz'))['test_x']).reshape([-1, 784])[:10]\n",
    "        ambiguous_dataset = torch.utils.data.TensorDataset(ambiguous_samples, torch.zeros(10))\n",
    "        ambiguous_loader = torch.utils.data.DataLoader(\n",
    "            ambiguous_dataset, batch_size=10, shuffle=False, drop_last=False\n",
    "        )\n",
    "        ambiguous_predicted_probabilities = model.predict(ambiguous_loader)\n",
    "        ambiguous_predicted_classes = np.argmax(ambiguous_predicted_probabilities, axis=1)\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = 5 * row // 2 + col\n",
    "                ax[row, col].imshow(np.reshape(ambiguous_samples[sample_idx], (28, 28)), cmap='gray')\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f'predicted {ambiguous_predicted_classes[sample_idx]}')\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(10), ambiguous_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
    "                )\n",
    "        fig.suptitle('Predictions on ambiguous and rotated MNIST', size=20)\n",
    "        fig.savefig(os.path.join(output_dir, 'ambiguous_rotated_mnist.pdf'))\n",
    "\n",
    "        # Do the same evaluation as on MNIST also on FashionMNIST\n",
    "        print('Predicting on FashionMNIST data')\n",
    "        fmnist_samples = torch.from_numpy(np.load(os.path.join(data_dir, 'fmnist.npz'))['x_test']).reshape([-1, 784])\n",
    "        fmnist_dataset = torch.utils.data.TensorDataset(fmnist_samples, torch.zeros(fmnist_samples.shape[0]))\n",
    "        fmnist_loader = torch.utils.data.DataLoader(\n",
    "            fmnist_dataset, batch_size=64, shuffle=False, drop_last=False\n",
    "        )\n",
    "        fmnist_predicted_probabilities = model.predict(fmnist_loader)\n",
    "        fmnist_predicted_classes = np.argmax(fmnist_predicted_probabilities, axis=1)\n",
    "        fmnist_confidences = np.max(fmnist_predicted_probabilities, axis=1)\n",
    "        fmnist_sorted_confidence_indices = np.argsort(fmnist_confidences)\n",
    "\n",
    "        # Plot FashionMNIST samples your model is most confident about\n",
    "        print('Plotting most confident FashionMNIST predictions')\n",
    "        most_confident_indices = fmnist_sorted_confidence_indices[-10:]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = most_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(np.reshape(fmnist_samples[sample_idx], (28, 28)), cmap='gray')\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f'predicted {fmnist_predicted_classes[sample_idx]}')\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(10), fmnist_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
    "                )\n",
    "        fig.suptitle('Most confident predictions', size=20)\n",
    "        fig.savefig(os.path.join(output_dir, 'fashionmnist_most_confident.pdf'))\n",
    "\n",
    "        # Plot FashionMNIST samples your model is least confident about\n",
    "        print('Plotting least confident FashionMNIST predictions')\n",
    "        least_confident_indices = fmnist_sorted_confidence_indices[:10]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = least_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(np.reshape(fmnist_samples[sample_idx], (28, 28)), cmap='gray')\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f'predicted {fmnist_predicted_classes[sample_idx]}')\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(10), fmnist_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
    "                )\n",
    "        fig.suptitle('Least confident predictions', size=20)\n",
    "        fig.savefig(os.path.join(output_dir, 'fashionmnist_least_confident.pdf'))\n",
    "\n",
    "        print('Determining suitability of your model for OOD detection')\n",
    "        all_confidences = np.concatenate([confidences, fmnist_confidences])\n",
    "        dataset_labels = np.concatenate([np.ones_like(confidences), np.zeros_like(fmnist_confidences)])\n",
    "        print(\n",
    "            'AUROC for MNIST vs. FashionMNIST OOD detection based on confidence: '\n",
    "            f'{roc_auc_score(dataset_labels, all_confidences):.3f}'\n",
    "        )\n",
    "        print(\n",
    "            'AUPRC for MNIST vs. FashionMNIST OOD detection based on confidence: '\n",
    "            f'{average_precision_score(dataset_labels, all_confidences):.3f}'\n",
    "        )\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple module implementing a feedforward neural network.\n",
    "    You can use this model as a reference/baseline for calibration\n",
    "    in the normal neural network case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_features: typing.Tuple[int, ...], out_features: int):\n",
    "        \"\"\"\n",
    "        Create a normal NN.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param hidden_features: Tuple where each entry corresponds to a hidden layer with\n",
    "            the corresponding number of features.\n",
    "        :param out_features: Number of output features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        feature_sizes = (in_features,) + hidden_features + (out_features,)\n",
    "        num_affine_maps = len(feature_sizes) - 1\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(feature_sizes[idx], feature_sizes[idx + 1], bias=True)\n",
    "            for idx in range(num_affine_maps)\n",
    "        ])\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        current_features = x\n",
    "\n",
    "        for idx, current_layer in enumerate(self.layers):\n",
    "            new_features = current_layer(current_features)\n",
    "            if idx < len(self.layers) - 1:\n",
    "                new_features = self.activation(new_features)\n",
    "            current_features = new_features\n",
    "\n",
    "        return current_features\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert x.shape[1] == 28 ** 2\n",
    "        estimated_probability = F.softmax(self.forward(x), dim=1)\n",
    "        assert estimated_probability.shape == (x.shape[0], 10)\n",
    "        return estimated_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a BayesNet model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fe7dcedc0b0>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model()\n",
    "m.network.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in m.network.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
